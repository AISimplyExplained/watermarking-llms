import { Callout } from 'nextra/components'
import CodeEditor from '../../components/editor'

# Getting Started
How about we start with a basic algorithm first for watermarking?
This follows the concept of <span style={{ color: 'green' }}>green</span> list and <span style={{ color: 'red' }}>red</span> list tokens.<br/><br/>
Let me explain a bit more, when the model is predicting the next token, it is not allowed to choose any of the tokens from the <span style={{ color: 'red' }}>red</span> list as the next token. <br/><br/>
How do we divide the tokens into <span style={{ color: 'red' }}>red</span> list and <span style={{ color: 'green' }}>green</span> list tokens you might ask?<br/>
<Callout emoji="âš ï¸">
    Python coming up
</Callout>
Let's make and run our own very basic simulation for this,
First up, let's define some variables that we really need which is our model's vocabulary and the prompt that we will send!
```python
vocabulary = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'runs', 'and']
prompt = "The quick brown"
```
Now, let's define our language_model function which will just assign random logits to each word for now and return the softmax probability vector for the purpose of the simulation.

```python
def language_model(vocabulary):
    # Assign random logits to each word in the vocabulary
    logits = np.random.randn(len(vocabulary))

    # Convert logits to probabilities using softmax
    probabilities = np.exp(logits) / np.sum(np.exp(logits))

    return dict(zip(vocabulary, probabilities))
```
Try running this cell below and see what it prints, you can even tinker with this code cell if you want<br/><br/>
<CodeEditor hiddenCode={"import numpy as np\nvocabulary = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'runs', 'and']\ndef language_model(vocabulary):\n\t# Assign random logits to each word in the vocabulary\n\tlogits = np.random.randn(len(vocabulary))\n\t# Convert logits to probabilities using softmax\n\tprobabilities = np.exp(logits) / np.sum(np.exp(logits))\n\treturn dict(zip(vocabulary, probabilities))"}>
{"# Apply the language model to get the probability vector\nprob_vector = language_model(vocabulary)\nprint(prob_vector)"}
</CodeEditor>

Now, we need to compute a hash. In simple terms, a hash is like a fingerprint for data. It is a unique string of characters generated by a mathematical algorithm from any input data. This string is typically of a fixed length, regardless of the size of the input. If the input to the hash function changes, the hash output also changes, but the same input will always give the same output.<br/>
Try it out for yourself ðŸ‘‡

<CodeEditor>
    {"import hashlib\ntoken='brown'\nprint(hashlib.sha256(token.encode()).hexdigest())"}
</CodeEditor>
Phew, a lot of work done already
Now let's use the value generated by the hash above to seed a random number generator

```python
random.seed(hash_value)
```
The generator is seeded with the last token, which enables the <span style={{ color: 'red' }}>red</span> list to be reproduced later without needing access to the entire generated text.
Now we will use the random number generator to partition the vocabulary into <span style={{ color: 'red' }}>red</span> list and <span style={{ color: 'green' }}>green</span> list tokens!

<CodeEditor hiddenCode={"import random\nimport hashlib\ntoken='brown'\nhash_value=hashlib.sha256(token.encode()).hexdigest()\nrandom.seed(hash_value)\nvocabulary = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'runs', 'and']\n"}>
    {"random.shuffle(vocabulary)\nhalf_size = len(vocabulary) // 2\ngreen_list = vocabulary[:half_size]\nred_list = vocabulary[half_size:]\nprint(\"red list: \", red_list)\nprint(\"green list: \", green_list)"}
</CodeEditor>

We will sample the next token from the green list, never generating any token in the red list. For now, let's pick the token from the green list with the highest probability.
Run this and see what you get!

<CodeEditor hiddenCode={"import numpy as np\nimport random\nimport hashlib\n\ndef language_model(vocabulary):\n\t# Assign random logits to each word in the vocabulary\n\tlogits = np.random.randn(len(vocabulary))\n\t# Convert logits to probabilities using softmax\n\tprobabilities = np.exp(logits) / np.sum(np.exp(logits))\n\treturn dict(zip(vocabulary, probabilities))\ntoken='brown'\nhash_value=hashlib.sha256(token.encode()).hexdigest()\nrandom.seed(hash_value)\nvocabulary = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'runs', 'and']\nprob_vector = language_model(vocabulary)\nrandom.shuffle(vocabulary)\nhalf_size = len(vocabulary) // 2\ngreen_list = vocabulary[:half_size]\nred_list = vocabulary[half_size:]\n"}>
    {"next_token = max(green_list, key=lambda word: prob_vector[word])\nprint(next_token)"}
</CodeEditor>

Cool, I think the simulation is pretty much complete. In the next section I have the full algorithm coded out.